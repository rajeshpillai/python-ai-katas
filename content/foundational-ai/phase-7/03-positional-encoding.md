# Positional Encoding

> Phase 7 â€” Attention & Transformers | Kata 7.3

---

## Concept & Intuition

### What problem are we solving?

<!-- TODO -->

### Why naive approaches fail

<!-- TODO -->

### Mental models

<!-- TODO -->

### Visual explanations

<!-- TODO -->

---

## Interactive Experiment

### Parameters
- Learning rate
- Epochs
- Layers
- Batch size

### Live Plots
- Loss
- Accuracy
- Gradients

---

## Live Code

```python
# TODO: starter code
```

---

## Key Takeaways

<!-- TODO -->
