# Python Backend — Todo

---

## Backend (FastAPI)

- [x] Set up FastAPI backend project structure
- [x] Secure code execution (sandboxed, not in main process)
- [x] CPU limits enforcement
- [x] Memory limits enforcement
- [x] Execution timeout enforcement
- [ ] Disable filesystem and network access in sandbox
- [x] Return metrics, visual outputs, and educational error messages
- [x] Progress tracking
- [x] Code versioning
- [x] Multi-track kata registry
- [ ] Dataset loading
- [ ] Simple authentication
- [ ] Anonymous users: temporary session, no persistence
- [ ] Logged-in users: saved progress, restored experiments, version history

---

# Foundational AI Track

---

## PHASE 0 — Foundations (Before ML)

- [x] Kata: What is data?
- [x] Kata: What is a feature?
- [x] Kata: What is noise?
- [x] Kata: What is signal?
- [x] Kata: Visualizing distributions

---

## PHASE 1 — What Does It Mean to Learn?

- [x] Kata: Constant predictor
- [x] Kata: Mean predictor
- [x] Kata: Distance-based prediction
- [x] Kata: Linear regression (from scratch, no PyTorch)
- [x] Teach: Loss functions
- [x] Teach: Error surfaces
- [x] Teach: Overfitting vs underfitting

---

## PHASE 2 — Optimization

- [x] Kata: Manual gradient descent
- [x] Kata: Learning rate experiments
- [x] Kata: Convergence vs divergence
- [x] Kata: Visualizing loss curves
- [x] Introduce: PyTorch tensors
- [x] Introduce: Autograd

---

## PHASE 3 — Artificial Neural Networks (ANN)

- [x] Kata: Single neuron
- [x] Kata: Multi-layer perceptron
- [x] Kata: Activation functions
- [x] Kata: Vanishing gradients
- [x] Kata: Dead neurons

---

## PHASE 4 — Representation Learning

- [x] Kata: Handcrafted vs learned features
- [x] Kata: PCA intuition
- [x] Kata: Embeddings
- [x] Kata: Visualizing hidden layers

---

## PHASE 5 — Convolutional Neural Networks (CNN)

- [x] Teach: Why dense networks fail on images
- [x] Teach: Convolution intuition
- [x] Teach: Filters as pattern detectors
- [x] Teach: Feature maps
- [x] Teach: Pooling effects
- [x] Dataset: MNIST integration
- [x] Dataset: CIFAR-10 integration

---

## PHASE 6 — Sequence Models

- [x] Teach: N-grams
- [x] Teach: RNN intuition
- [x] Teach: LSTM limitations
- [x] Teach: Why recurrence struggles at scale

---

## PHASE 7 — Attention & Transformers

- [x] Teach: Attention as weighted averaging
- [x] Teach: Self-attention visualization
- [x] Teach: Positional encoding
- [x] Teach: Why transformers scale

---

## PHASE 8 — Large Language Models (LLMs)

- [x] Teach: Tokenization
- [x] Teach: Next-token prediction
- [x] Teach: Sampling strategies
- [x] Teach: Temperature, top-k, top-p
- [x] Teach: Emergence through scale

---

## PHASE 9 — Reasoning Models

- [x] Teach: Chain-of-thought as latent variables
- [x] Teach: Scratchpads
- [x] Teach: Tool usage
- [x] Teach: Planning vs prediction
- [x] Teach: Failure modes of LLM reasoning

---

# Traditional AI/ML Track

---

## PHASE 0 — What is AI? (5 katas)

- [x] Kata: What is AI?
- [x] Kata: Rule-based systems
- [x] Kata: Search algorithms
- [x] Kata: Heuristics and cost
- [x] Kata: Knowledge representation

---

## PHASE 1 — Data Wrangling (7 katas)

- [x] Kata: Tabular data basics
- [x] Kata: Missing values
- [x] Kata: Outlier detection
- [x] Kata: Encoding categorical variables
- [x] Kata: Feature scaling
- [x] Kata: Train-test split
- [x] Kata: Exploratory data analysis

---

## PHASE 2 — Supervised Learning: Regression (7 katas)

- [x] Kata: Simple linear regression
- [x] Kata: Multiple linear regression
- [x] Kata: Polynomial regression
- [x] Kata: Regularization: Ridge
- [x] Kata: Regularization: Lasso
- [x] Kata: Elastic Net
- [x] Kata: Regression diagnostics

---

## PHASE 3 — Supervised Learning: Classification (7 katas)

- [x] Kata: K-nearest neighbors
- [x] Kata: Logistic regression
- [x] Kata: Decision trees
- [x] Kata: Support vector machines
- [x] Kata: Naive Bayes
- [x] Kata: Multiclass strategies
- [x] Kata: Imbalanced classes

---

## PHASE 4 — Model Evaluation & Selection (6 katas)

- [x] Kata: Accuracy and its limits
- [x] Kata: Precision, recall, F1
- [x] Kata: ROC and AUC
- [x] Kata: Cross-validation
- [x] Kata: Hyperparameter tuning
- [x] Kata: Bias-variance tradeoff

---

## PHASE 5 — Unsupervised Learning (6 katas)

- [x] Kata: K-means clustering
- [x] Kata: Hierarchical clustering
- [x] Kata: DBSCAN
- [x] Kata: PCA for dimensionality reduction
- [x] Kata: t-SNE visualization
- [x] Kata: Anomaly detection

---

## PHASE 6 — Ensemble Methods (7 katas)

- [x] Kata: Bagging intuition
- [x] Kata: Random forests
- [x] Kata: Boosting intuition
- [x] Kata: AdaBoost
- [x] Kata: Gradient boosting
- [x] Kata: XGBoost
- [x] Kata: Stacking

---

## PHASE 7 — Feature Engineering & Pipelines (6 katas)

- [x] Kata: Feature creation
- [x] Kata: Feature selection
- [x] Kata: Text features
- [x] Kata: Datetime features
- [x] Kata: Scikit-learn pipelines
- [x] Kata: Custom transformers

---

## PHASE 8 — Time Series & Sequential Data (6 katas)

- [x] Kata: Time series basics
- [x] Kata: Autocorrelation
- [x] Kata: ARIMA
- [x] Kata: Exponential smoothing
- [x] Kata: Forecasting evaluation
- [x] Kata: Sequence classification

---

## PHASE 9 — Reinforcement Learning (7 katas)

- [x] Kata: Markov decision processes
- [x] Kata: Value functions
- [x] Kata: Policy iteration
- [x] Kata: Q-learning
- [x] Kata: SARSA
- [x] Kata: Multi-armed bandits
- [x] Kata: Environment design

---

## PHASE 10 — Probabilistic & Bayesian Methods (5 katas)

- [x] Kata: Bayesian thinking
- [x] Kata: Maximum likelihood estimation
- [x] Kata: MAP estimation
- [x] Kata: Gaussian mixture models
- [x] Kata: Bayesian optimization

---

## PHASE 11 — Productionizing ML (6 katas)

- [x] Kata: Model serialization
- [x] Kata: Data drift detection
- [x] Kata: Experiment tracking
- [x] Kata: A/B testing for models
- [x] Kata: Interpretability
- [x] Kata: Responsible AI
